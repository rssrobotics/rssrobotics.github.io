---
layout: page
title: Area Chair Symposium
description: Area chair symposium time and details.
invisible: true
---

<!--
The RSS Area Chair Symposium will be held in Gates Hall G01 at Cornell University in Ithaca, New York, USA on Thursday, April 5, 2018.

<iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d5888.397471047109!2d-76.4849788063598!3d42.44478745926767!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x89d0818b5e725517%3A0x7098683a6a981542!2sBill+and+Melinda+Gates+Hall!5e0!3m2!1sen!2sus!4v1522680351301" width="600" height="450" frameborder="0" style="border:0" allowfullscreen></iframe>

## Program Summary

<table class="table">
    <tbody class="text-left">
      <tr>
        <td style="width: 130px">09:00 - 10:30</td>
        <td>
          <b>Design/Software</b> <br/>
          Andrea Censi, ETH Zürich & nuTonomy, inc., <a href="javascript:void($('#censi').toggle());"><i>The AI Driving Olympics</i></a>
          <div id="censi" style="display:none;">
            <br/>
            I will describe the first AI Driving Olympics (AIDO), organized by
            ETH Zurich, University of Montreal, Georgia Tech, Tsinghua, NCTU, nuTonomy, and
            Amazon. The AIDO are a "live competition" at NIPS 2018 which will run in the
            period October-December 2018. Participants compete on a wide range of
            driving-related tasks on the Duckietown platform that are meant to understand
            the potential and limitations of model-based and learning-based approaches.
            Algorithms are tested on remote Duckietown robotariums built at the
            participating universities. The finals will happen in Montreal and will be
            streamed live.
            <br>
            <br/>
            In other words: the Duckies go to NIPS!
          </div>
          <br/>
          Hanna Kurniawati, University of Queensland, <a href="javascript:void($('#kurniawati').toggle());"><i>Software for POMDP-based motion planning</i></a>
          <div id="kurniawati" style="display:none;">
            <br/>
            I will talk about our new software for POMDP-based motion planning.
            It allows users to provide configuration file to describe problems
            (rather than programming them inside the solver). It interface to
            ROS-Gazebo.
          </div>
          <br/>
          Justus Piater, University of Innsbruck, <a href="javascript:void($('#piater').toggle());"><i>High-Level, Skill-Based Robot Programming Using Autonomous Playing and Autonomous Testing</i></a>
          <div id="piater" style="display:none;">
            <br/>
            We introduce a novel paradigm for robot programming with which we
            aim to make robot programming more accessible for unexperienced
            users. In order to do so we incorporate two major components in one
            single framework: autonomous skill acquisition by robotic playing
            and visual programming. Simple robot program skeletons solving a
            task for one specific situation, so-called basic behaviours, are
            provided by the user. The robot then learns how to solve the same
            task in many different situations by autonomous playing which
            reduces the barrier for unexperienced robot programmers. Programmers
            can use a mix of visual programming and kinesthetic teaching in
            order to provide these simple program skeletons. The robot program
            can be implemented interactively by programming parts with visual
            programming and kinesthetic teaching. We further integrate work on
            experience-based skill-centric robot software testing which enables
            the user to continuously test implemented skills without having to
            deal with the details of specific components.
          </div>
          <br/>
          Jason O'Kane, University of South Carolina, <a href="javascript:void($('#okane').toggle());"><i>Toward Automated Design Tools for Minimal Robots</i></a>
          <div id="okane" style="display:none;">
            <br/>
            Designing robot systems (or even just software for robots) is hard! In this
            talk, I will describe some recent preliminary research on the plausibility of
            using automated tools to assist human roboticists with that process.
          </div>
          <br/>
          Mateo Bianchi, University of Pisa, <a href="javascript:void($('#bianchi').toggle());"><i>Bio-aware design of robotic systems and haptic-interfaces</i></a>
          <div id="bianchi" style="display:none;">
            <br/>
            The human body is an extraordinarily sophisticated and versatile sensorimotor
            system. Controlling its large number of elements, such as muscles, bones, and
            joints, as well as integrating multiple sensory modalities, are complex tasks
            the Central Nervous System (CNS) must deal with. Trying to replicate such a rich
            variety of behavior in robotic and haptic systems is a daunting task and merely
            copy-catting biological observations in an artificial body is clearly
            unfeasible. On the contrary, bio-aware robotics can represent a successful
            approach to fully take advantage from nature for a new generation of
            technological devices. A possible strategy to pursue this objective is the
            translation of neuroscientific results into a mathematical language, which can
            be understood by artificial systems and used to inform a more effective device
            design. In this talk, I will discuss how neuroscience can inspire bio-aware
            robotics, leveraging on the concept of sensory-motor synergies as a generalized
            simplification approach humans rely on to cope with the abundancy of body
            sensors and degrees of freedom. I will discuss how this concept can be
            successfully applied to inform the development and control of simple, intuitive
            yet effective robotic systems, with special focus on haptics, tactile sensing
            and wearability. Applications and perspectives for robotics-enabled human
            assistance, prosthetics and advanced human-robot interaction will be finally
            discussed.
          </div>
          <br/>
          Eiichi Yoshida, National Institute of Advanced Industrial Science and Technology, <a href="javascript:void($('#yoshida').toggle());"><i>Unified Motion Synthesis of Digital Human and Humanoid for Product Design and Evaluation</i></a>
          <div id="yoshida" style="display:none;">
          We present an integrated approach to motion synthesis for humanoid
          robotics and human simulation that is useful for product design and
          evaluation. The first axis is development of a method for humanoid
          robot control that can reproduce various human behaviors to use a
          humanoid robot as an evaluator of products such as assistive devices.
          This allows estimating its mechanical supportive effects in a
          quantitative manner, which is difficult with human measurement. We
          also introduce applications of this research to standardization of
          wearable lumbar-support assistive devices. Another main research
          direction is to develop a system for human-centered product design
          through understanding humans' motion principles by using a digital
          human that can model its shape, musculo-skeletal structure and
          motions, as well as interactions with devices. We will show some
          applications of product design and evaluation based on this research.
          <br/>
          </div>
        </td>
      </tr>
      <tr>
        <td>10:30 - 11:00</td>
        <td>
          <b>Coffee Break</b>
        </td>
      </tr>
      <tr>
        <td>11:00 - 12:30</td>
        <td>
          <b>Manipulation/Wearables/Multi robot</b> <br/>
          Mehmet Dogar, University of Leeds, <a href="javascript:void($('#dogar').toggle());"><i>Manipulation Planning for Forceful Human-Robot Collaboration</i></a>
          <div id="dogar" style="display:none;">
            <br/>
            Imagine a human and a robot collaborating to cut, drill, and
            assemble parts to build a table. I will present our work on
            manipulation planning in such a scenario, where the robot grasps an
            object on which a human is applying sequential and changing forces,
            e.g. cutting pieces off the parts, then drilling a few holes, then
            inserting fasteners to those holes, etc. I will try to answer two
            questions: (i) Given such a collaboration task, how can the robot
            plan an efficient sequence of grasps and motions that are stable
            against these forces? (ii) How can the robot make sure that the
            forceful tasks are executed comfortably by the human?Imagine a human
            and a robot collaborating to cut, drill, and assemble parts to build
            a table. I will present our work on manipulation planning in such a
            scenario, where the robot grasps an object on which a human is
            applying sequential and changing forces, e.g. cutting pieces off the
            parts, then drilling a few holes, then inserting fasteners to those
            holes, etc. I will try to answer two questions: (i) Given such a
            collaboration task, how can the robot plan an efficient sequence of
            grasps and motions that are stable against these forces? (ii) How
            can the robot make sure that the forceful tasks are executed
            comfortably by the human?
          </div>
          <br/>
          Sami Haddadin, Technische Universität München, <a href="javascript:void($('#haddadin').toggle());"><i>Learning Dynamics and Manipulation Control</i></a>
          <div id="haddadin" style="display:none;">
            <br/>
            I will talk about first order principle networks for learning
            accurate and interpretable dynamics as well as learning human-level
            performance manipulation skills
          </div>
          <br/>
          David Braun, Singapore University of Technology and Design, <a href="javascript:void($('#braun').toggle());"><i>Analytical Design and Compliant Actuation for Autonomous Human Augmentation</i></a>
          <div id="braun" style="display:none;">
            <br/>
            We present an integrated approach to motion synthesis for humanoid
            robotics and human simulation that is useful for product design and
            evaluation. The first axis is development of a method for humanoid
            robot control that can reproduce various human behaviors to use a
            humanoid robot as an evaluator of products such as assistive
            devices. This allows estimating its mechanical supportive effects in
            a quantitative manner, which is difficult with human measurement. We
            also introduce applications of this research to standardization of
            wearable lumbar-support assistive devices. Another main research
            direction is to develop a system for human-centered product design
            through understanding humans' motion principles by using a digital
            human that can model its shape, musculo-skeletal structure and
            motions, as well as interactions with devices. We will show some
            applications of product design and evaluation based on this
            research.
          </div>
          <br/>
          Elliott Rouse, University of Michigan, <a href="javascript:void($('#rouse').toggle());"><i>The hidden mechanics of human locomotion</i></a>
          <div id="rouse" style="display:none;">
            <br/>
            Brief overview of the Neurobionics Lab. Our group focuses on
            estimating the mechanical impedance of the joints of the legs, and
            incorporating these properties in a new generation of wearable
            robotic technologies.
          </div>
          <br/>
          Ani Hsieh, University of Pennsylvania, <a href="javascript:void($('#hsieh').toggle());"><i>TBD</i></a>
          <div id="hsieh" style="display:none;">
            <br/>
            Overview of our operating autonomous systems in geophysical flows.
          </div>
          <br/>
          Kirstin Petersen, Cornell University, <a href="javascript:void($('#petersen').toggle());"><i>Designing Robot Collectives</i></a>
          <div id="petersen" style="display:none;">
            <br/>
            Brief overview of the work we do in the Collective Embodied Intelligence Lab
          </div>
        </td>
      </tr>
      <tr>
        <td>12:30 - 13:15</td>
        <td>
          <b>Lunch</b>
        </td>
      </tr>
      <tr>
        <td>13:15 - 14:30</td>
        <td>
          <b>Lab Tours</b>
        </td>
      </tr>
      <tr>
        <td>14:45 - 16:00</td>
        <td>
          <b>Deep Learning</b> <br/>
          Jens Kober, Delft University of Technology, <a href="javascript:void($('#kober').toggle());"><i>Efficient Deep RL for Robotics</i></a>
          <div id="kober" style="display:none;">
            <br/>
            Deep reinforcement learning has been extremely successful. However,
            when applying it to real robot control tasks, there a a lot of
            stumbling blocks. This talk will present some ideas on rendering the
            learning process sample-efficient and capable of dealing with
            limited replay buffer sizes.
          </div>
          <br/>
          Matthew Johnson-Roberson, University of Michigan, <a href="javascript:void($('#roberson').toggle());"><i>Deep learning for Field Robots: Stop Hand Labeling</i></a>
          <div id="roberson" style="display:none;">
            <br/>
            A discussion of techniques to limit the amount of data labeling required to deploy machine learning on field robotic systems.
          </div>
          <br/>
          Tucker Hermans, University of Utah, <a href="javascript:void($('#hermans').toggle());"><i>Planning Multi-Fingered Grasps as Probabilistic Inference in a Learned Deep Network</i></a>
          <div id="hermans" style="display:none;">
            <br/>
            Learning complex cost functions for use in optimization-based motion
            planning.
          </div>
          <br/>
          Jonathan Kelly, University of Toronto Institute for Aerospace Studies, <a href="javascript:void($('#kelly').toggle());"><i>Sunny Days: Improving Visual Odometry Using Deep Visual Illumination Estimation</i></a>
          <div id="kelly" style="display:none;">
            <br/>
            Visual navigation is essential for many successful robotics
            applications. Visual odometry (VO), an incremental dead reckoning
            technique, in particular, has been widely employed on many
            platforms, including the Mars Exploration Rovers and the Mars
            Science Laboratory. However, a drawback of this visual motion
            estimation approach is that it exhibits superlinear growth in
            positioning error with time, due in large part to orientation drift.
            In this talk, I will describe recent work in our group on a method
            to incorporate global orientation information from the sun into a
            visual odometry (VO) pipeline, using data from the existing image
            stream only. This is challenging in part because the sun is
            typically not visible in the input images. Our work leverages recent
            advances in Bayesian Convolutional Neural Networks (BCNNs) to train
            and implement a sun detection model (dubbed Sun-BCNN) that infers a
            three-dimensional sun direction vector from a single RGB image.
            Crucially, the technique also computes a principled uncertainty
            associated with each prediction, using a Monte Carlo dropout scheme.
            We incorporate this uncertainty into a sliding window stereo VO
            pipeline where accurate uncertainty estimates are critical for
            optimal data fusion.
          </div>
          <br/>
          Matthew Walter, Toyota Technological Institute at Chicago, <a href="javascript:void($('#walter').toggle());"><i>Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning</i></a>
          <div id="walter" style="display:none;">
            <br/>
            Describe a method that jointly optimizes over a robot's physical
            design and the corresponding control policy in a model-free fashion,
            without the need for expert supervision.
          </div>
        </td>
      </tr>
      <tr>
        <td>16:00 - 16:30</td>
        <td>
          <b>Coffee Break</b>
        </td>
      </tr>
      <tr>
        <td>16:30 - 17:15</td>
        <td>
          <b>HRI</b> <br/>
          Laurel Riek, University of California, San Diego, <a href="javascript:void($('#riek').toggle());"><i>Fluent Human Robot Teaming in Safety Critical Environments</i></a>
          <div id="riek" style="display:none;">
            <br/>
            To operate proximately with people, robots need the ability to
            dynamically and quickly interpret human activities, understand
            context, and take appropriate (and safe) actions. They also need to
            learn from and adapt to people long term. Our research focuses on
            building robots that autonomously solve problems in human
            environments, particularly those that are safety critical (e.g.,
            hospitals, homes, and factories). Recent contributions include new
            methods to model stochastic environments and circumvent sensor noise
            and occlusion, new techniques to enable robots to robustly solve
            problems under limited computational resources, and methods for
            robots to perceive and learn from people long term. Our primary
            application focus is healthcare, with recent work in emergency
            medicine and neurorehabilitation. This talk will describe several
            recent projects in this space.
          </div>
          <br/>
          Dana Kulic, University of Waterloo, <a href="javascript:void($('#kulic').toggle());"><i>Human Supervision of Autonomy</i></a>
          <div id="kulic" style="display:none;">
            <br/>
            As robot capabilities increase, many robots are becoming capable of
            autonomous behaviour in a broad range of environments, but rely on
            human supervision and direction. In particular, human operators
            should specify goals, constraints and other guidelines for robot
            behaviour. Novice human operators, who may not be familiar with
            robot capabilities and behaviours, may find it difficult to specify
            tasks appropriately, and may not appreciate the impact of the
            specifications on task performance. In this talk, I will review
            recent work on interactive task specification and learning user
            preferences through interaction.
          </div>
          <br/>
          Maya Cakmak, University of Washington, <a href="javascript:void($('#cakmak').toggle());"><i>TBD</i></a>
          <div id="cakmak" style="display:none;">
            <br/>
            TBD
          </div>
          </td>
      </tr>
      <td>
      </td>
      <td></td>
    </tbody>
</table>
-->
